{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Object Detection and Segmentation with YOLOv8\n\nThis notebook uses YOLOv8 to perform object detection and segmentation on keyframes from videos, using COCO dataset classes.\n\n## 1. Install required libraries","metadata":{"_uuid":"aca48553-8043-46a6-a5c9-07cda69c1ef5","_cell_guid":"cbe4b26b-d165-4f50-b6ed-892edf14e858","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Install required libraries\nimport sys\nimport subprocess\n\n# Install packages using subprocess\n!pip install -q torch torchvision\n!pip install -q ultralytics\n!pip install -qopencv-python-headless\n!pip install -qgdown\n!pip install -qmatplotlib\n!pip install -qPillow","metadata":{"_uuid":"5edcd7b7-b924-43c5-b75b-41d095b04c23","_cell_guid":"defff61a-b1da-4b32-b8a8-80d9d3917304","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-20T03:33:43.764481Z","iopub.execute_input":"2025-07-20T03:33:43.764651Z","iopub.status.idle":"2025-07-20T03:35:05.586654Z","shell.execute_reply.started":"2025-07-20T03:33:43.764634Z","shell.execute_reply":"2025-07-20T03:35:05.585619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Download data from Google Drive","metadata":{"_uuid":"7a6081f2-840a-40f6-9c2c-6d3b769795a6","_cell_guid":"5b931c9c-0d89-4721-ac6c-fdde485462ef","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Configure batch and Google Drive IDs\nBATCH_NAME = \"L01\"  # Batch name (L01, L02, L03, ...)\nBATCH_ID = \"14MeYV2WBWwldMDGRrpG9s7vz8triwbWr\"  # ID for L01.zip\n\n# Create data directory if it doesn't exist\nimport os\nif not os.path.exists('data'):\n    os.makedirs('data', exist_ok=True)\n\n# Download batch (keyframes) from Google Drive\nprint(f\"Downloading batch {BATCH_NAME}...\")\nimport gdown\ngdown.download(id=BATCH_ID, output=f\"data/{BATCH_NAME}.zip\", quiet=True)\n\n# Unzip the downloaded file\nimport zipfile\nwith zipfile.ZipFile(f\"data/{BATCH_NAME}.zip\", 'r') as zip_ref:\n    zip_ref.extractall('./')\n\nprint(\"Data downloaded successfully!\")\n\n# Create directory for detection results\nif not os.path.exists('detection_results'):\n    os.makedirs('detection_results', exist_ok=True)\n\nprint(\"Data downloaded successfully!\")","metadata":{"_uuid":"adf8cc38-a3af-4a16-8218-13ca60558765","_cell_guid":"0f762125-bfc7-4c6c-b753-e3ef16a98fa7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-20T03:35:13.705906Z","iopub.execute_input":"2025-07-20T03:35:13.706467Z","iopub.status.idle":"2025-07-20T03:35:24.284087Z","shell.execute_reply.started":"2025-07-20T03:35:13.706436Z","shell.execute_reply":"2025-07-20T03:35:24.283185Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Import libraries","metadata":{"_uuid":"94d5dab8-7f7c-47db-b9c0-47af3b286e7c","_cell_guid":"9877c654-791b-49fe-b476-18ab282c3bab","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Import required libraries\nimport os\nimport json\nimport glob\nimport torch\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nfrom ultralytics import YOLO\n# Custom JSON encoder to handle numpy types\nclass NumpyEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        if isinstance(obj, np.floating):\n            return float(obj)\n        if isinstance(obj, np.ndarray):\n            return obj.tolist()\n        return super(NumpyEncoder, self).default(obj)\n\n# Handle Google Drive connection if running in Colab\ntry:\n    from google.colab import drive\n    try:\n        drive.mount('/content/drive')\n        print(\"[INFO] Google Drive connected via Colab\")\n    except NotImplementedError:\n        print(\"[INFO] Current environment does not support mounting Google Drive via Colab\")\n        # In Kaggle, data is accessed directly from the current directory\nexcept ImportError:\n    print(\"[INFO] Not running in Colab, skipping Google Drive mount\")","metadata":{"_uuid":"4367b649-b5b2-4383-bb8d-77eab53917db","_cell_guid":"73e60bcd-5595-45b7-96b0-7f9b5b469ad7","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-20T03:35:28.235096Z","iopub.execute_input":"2025-07-20T03:35:28.235931Z","iopub.status.idle":"2025-07-20T03:35:32.094713Z","shell.execute_reply.started":"2025-07-20T03:35:28.235906Z","shell.execute_reply":"2025-07-20T03:35:32.094002Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Check environment","metadata":{"_uuid":"b7c87530-9793-4970-a54f-c7b1357ab39e","_cell_guid":"dd81bd29-6769-44ce-a765-98f24b03158b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Check GPU\nif torch.cuda.is_available():\n    device_count = torch.cuda.device_count()\n    current_device = torch.cuda.current_device()\n    device_name = torch.cuda.get_device_name(current_device)\n    \n    print(f\"Number of GPUs: {device_count}\")\n    print(f\"Current GPU: {current_device}\")\n    print(f\"GPU name: {device_name}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\nelse:\n    print(\"CUDA not available. Check NVIDIA drivers and PyTorch CUDA installation.\")","metadata":{"_uuid":"8a38aeae-e951-46b6-a771-e0d94463db66","_cell_guid":"0891e124-677d-4eb0-b489-f623002a054e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-20T03:35:36.334170Z","iopub.execute_input":"2025-07-20T03:35:36.334976Z","iopub.status.idle":"2025-07-20T03:35:36.444827Z","shell.execute_reply.started":"2025-07-20T03:35:36.334949Z","shell.execute_reply":"2025-07-20T03:35:36.444260Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Configure batch and video indices","metadata":{"_uuid":"5cec13ab-65ea-41ea-b4fa-a3f2c23d897f","_cell_guid":"d0cc8893-3b56-40b2-9cad-ff5f02fab4ba","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Configure batch processing\nSTART_VIDEO_INDEX = 1  # Start from V001\nBATCH_SIZE = 8  # Process 8 videos at a time\n\n# Define paths\nBATCH_PATH = BATCH_NAME  # Example: \"L01\"\n\n# Get list of videos in the batch\nvideos = sorted(glob.glob(os.path.join(BATCH_PATH, \"V*\")))\n\n# Define paths\nBATCH_PATH = BATCH_NAME  # Example: \"L01\"\n\n# Get list of videos in the batch\nvideos = sorted(glob.glob(os.path.join(BATCH_PATH, \"V*\")))\nprint(f\"Found {len(videos)} video directories in batch {BATCH_PATH}\")\n\n# Only process videos from START_VIDEO_INDEX to START_VIDEO_INDEX + BATCH_SIZE - 1\nend_idx = min(START_VIDEO_INDEX + BATCH_SIZE - 1, len(videos))\nselected_videos = videos[START_VIDEO_INDEX - 1:end_idx]\nprint(f\"Processing {len(selected_videos)} videos: {[os.path.basename(v) for v in selected_videos]}\")","metadata":{"_uuid":"76352098-b54b-4c6a-b600-b7b3e00b54df","_cell_guid":"82afbbac-5529-4fb6-a4f0-6f2da683f9ff","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-20T03:35:42.351539Z","iopub.execute_input":"2025-07-20T03:35:42.352370Z","iopub.status.idle":"2025-07-20T03:35:42.359024Z","shell.execute_reply.started":"2025-07-20T03:35:42.352343Z","shell.execute_reply":"2025-07-20T03:35:42.358308Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6. Load YOLOv8 model","metadata":{"_uuid":"3aff3854-f625-4fe7-9138-b747414a1789","_cell_guid":"76a12cd8-3d49-4bdf-8160-a922026de2cf","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load YOLOv8 model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load the YOLOv8 model with segmentation capabilities\nmodel = YOLO(\"yolov8x-seg.pt\")  # Use x-seg for best performance with segmentation\n\nprint(f\"YOLOv8 model loaded on device: {device}\")\n\n# Display the model information and COCO classes\nprint(f\"Model info: YOLOv8 using COCO dataset with {len(model.names)} classes\")\nprint(f\"COCO class names: {model.names}\")","metadata":{"_uuid":"cd680d4a-616c-4ddb-9082-e5906005a4c2","_cell_guid":"b92d0916-0149-47c8-98ed-5755c5210a5c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-20T03:35:45.112563Z","iopub.execute_input":"2025-07-20T03:35:45.113318Z","iopub.status.idle":"2025-07-20T03:35:48.005202Z","shell.execute_reply.started":"2025-07-20T03:35:45.113293Z","shell.execute_reply":"2025-07-20T03:35:48.004418Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Helper functions for object detection and filtering","metadata":{"_uuid":"e2ca66fb-381a-4205-9cb7-532257e64cd3","_cell_guid":"ac3639e9-46b8-4fb6-a943-5471a7b7c0c2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Function to detect objects with YOLOv8\ndef detect_objects(image_path, conf_threshold=0.35):\n    \"\"\"\n    Detect objects in an image using YOLOv8 with COCO classes\n    \n    Returns dictionary with boxes, scores, classes and segmentation masks\n    \"\"\"\n    try:\n        # Run YOLOv8 inference on the image\n        results = model(image_path, conf=conf_threshold)\n        \n        # Process results\n        result = results[0]  # Get the first result (only one image)\n        \n        # Extract boxes, convert from xyxy format to [x1, y1, x2, y2]\n        boxes = []\n        scores = []\n        class_names = []\n        \n        # Extract detection information\n        for box, cls, conf in zip(result.boxes.xyxy.cpu().numpy(), \n                                 result.boxes.cls.cpu().numpy(), \n                                 result.boxes.conf.cpu().numpy()):\n            boxes.append(box.tolist())\n            scores.append(conf)\n            class_name = model.names[int(cls)]\n            class_names.append(class_name)\n        \n        return {\n            \"boxes\": boxes,\n            \"scores\": scores, \n            \"labels\": class_names\n        }\n        \n    except Exception as e:\n        print(f\"Error processing {os.path.basename(image_path)}: {e}\")\n        return {\n            \"boxes\": [],\n            \"scores\": [],\n            \"labels\": []\n        }\n\n# Function to visualize detection results on an image\ndef visualize_detection(image_path, detection_results):\n    # Check if the image file exists\n    if not os.path.exists(image_path):\n        print(f\"Error: Image file does not exist: {image_path}\")\n        return None\n    \n    # Try to read the image\n    image = cv2.imread(image_path)\n    \n    # Check if image was successfully loaded\n    if image is None:\n        print(f\"Error: Failed to load image: {image_path}\")\n        return None\n    \n    try:\n        # Convert color space\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        for box, score, label in zip(detection_results[\"boxes\"], detection_results[\"scores\"], detection_results[\"labels\"]):\n            x1, y1, x2, y2 = map(int, box)\n            \n            # Draw bounding box\n            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n            \n            # Draw label and score\n            text = f\"{label}: {score:.2f}\"\n            cv2.putText(image, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n        \n        plt.figure(figsize=(12, 8))\n        plt.imshow(image)\n        plt.axis('off')\n        plt.show()\n        \n        return image\n    except Exception as e:\n        print(f\"Error processing image {image_path}: {e}\")\n        return None\n# Function to calculate IoU (Intersection over Union)\ndef calculate_iou(box1, box2):\n    \"\"\"Calculate IoU between two bounding boxes\"\"\"\n    # Box coordinates\n    x1_1, y1_1, x2_1, y2_1 = box1\n    x1_2, y1_2, x2_2, y2_2 = box2\n    \n    # Calculate area of each box\n    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n    \n    # Calculate coordinates of intersection\n    x1_i = max(x1_1, x1_2)\n    y1_i = max(y1_1, y1_2)\n    x2_i = min(x2_1, x2_2)\n    y2_i = min(y2_1, y2_2)\n    \n    # Check if there is no intersection\n    if x2_i < x1_i or y2_i < y1_i:\n        return 0.0\n    \n    # Calculate area of intersection\n    area_intersection = (x2_i - x1_i) * (y2_i - y1_i)\n    \n    # Calculate IoU\n    iou = area_intersection / (area1 + area2 - area_intersection)\n    \n    return iou\n\n# Function to filter duplicate objects and keep only the highest scoring object\ndef filter_objects(objects, iou_threshold=0.7, confidence_threshold=0.5):\n    \"\"\"Filter duplicated objects, keep only the highest scoring object for each group of overlapping boxes\"\"\"\n    # If there are no objects, return empty list\n    if not objects:\n        return []\n    \n    # Filter objects based on confidence threshold\n    objects = [obj for obj in objects if obj[\"score\"] >= confidence_threshold]\n    \n    # Sort objects by score in descending order\n    sorted_objects = sorted(objects, key=lambda x: x[\"score\"], reverse=True)\n    \n    # List to store filtered objects\n    filtered_objects = []\n    \n    # Iterate through each object\n    for obj in sorted_objects:\n        # Check if current object overlaps with any object in filtered_objects\n        duplicate = False\n        for filtered_obj in filtered_objects:\n            # If same object name and IoU greater than threshold\n            if obj[\"object\"] == filtered_obj[\"object\"] and \\\n               calculate_iou(obj[\"box\"], filtered_obj[\"box\"]) > iou_threshold:\n                duplicate = True\n                break\n        \n        # If not duplicate, add to filtered list\n        if not duplicate:\n            filtered_objects.append(obj)\n    \n    return filtered_objects\n\n# Load YOLOv8 model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Load the YOLOv8 model with segmentation capabilities\nmodel = YOLO(\"yolov8x-seg.pt\")  # Use x-seg for best performance with segmentation\n\nprint(f\"YOLOv8 model loaded on device: {device}\")\n\n# Display the model information and COCO classes\nprint(f\"Model info: YOLOv8 using COCO dataset with {len(model.names)} classes\")\nprint(f\"COCO class names: {model.names}\")\n\n# Process each video\nfor video_dir in selected_videos:\n    video_name = os.path.basename(video_dir)\n    print(f\"\\nProcessing video: {video_name}\")\n    \n    # Directly find keyframes from directories\n    keyframe_dir = f\"{BATCH_NAME}/{video_name}\"\n    print(f\"Looking for keyframes in directory: {keyframe_dir}\")\n    \n    # List all JPG files in the keyframe directory\n    keyframe_files = glob.glob(os.path.join(keyframe_dir, \"*.jpg\"))\n    \n    if not keyframe_files:\n        print(f\"No keyframes found for {video_name} in {keyframe_dir}\")\n        continue\n        \n    print(f\"Found {len(keyframe_files)} keyframes in {keyframe_dir}\")\n    \n    # Initialize list to store results\n    detection_results = []\n    \n    # Process each keyframe\n    for keyframe_path in keyframe_files:\n        keyframe_name = os.path.basename(keyframe_path)\n        \n        print(f\"Processing keyframe: {keyframe_name}\")\n        \n        # Initialize results for current keyframe\n        keyframe_results = {\n            \"keyframe\": keyframe_name,\n            \"caption\": \"\",  # Empty caption since we're not using caption files\n            \"objects\": []\n        }\n        \n        # Run YOLOv8 detection with COCO classes\n        results = detect_objects(keyframe_path)\n        \n        # Add results to the list\n        for i, (box, score, label) in enumerate(zip(results[\"boxes\"], results[\"scores\"], results[\"labels\"])):\n            keyframe_results[\"objects\"].append({\n                \"prompt\": \"COCO classes\",  # Using all COCO classes instead of prompts\n                \"object\": label,\n                \"box\": box,\n                \"score\": score\n            })\n        \n        # Apply filter_objects to remove duplicate objects and filter by score\n        keyframe_results[\"objects\"] = filter_objects(keyframe_results[\"objects\"])\n        \n        print(f\"Keyframe {keyframe_name}: {len(keyframe_results['objects'])} objects after filtering\")\n        \n        # Add keyframe results to the main list\n        detection_results.append(keyframe_results)\n    \n    # Save results to JSON file\n    output_file = os.path.join(\"detection_results\", f\"{BATCH_NAME}_{video_name}_detection_yolov8.json\")\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(detection_results, f, ensure_ascii=False, indent=4, cls=NumpyEncoder)\n    \n    print(f\"\\nSaved detection results for {video_name} to {output_file}\")\n\nprint(\"\\nObject detection completed for all videos!\")","metadata":{"_uuid":"5d759346-573f-483f-a588-581e94f7d040","_cell_guid":"5e3fab79-0dee-4734-9ba8-b2f9da9e467b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-20T03:36:58.789932Z","iopub.execute_input":"2025-07-20T03:36:58.790734Z","execution_failed":"2025-07-20T03:37:32.773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Visualize some results","metadata":{"_uuid":"4f2a17c1-b015-451e-b8cb-d6b6554947eb","_cell_guid":"f96f3395-9d7e-46c0-805c-dd5b773ec9b1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Visualize some detection results (a random example)\n# Choose a random keyframe from the results\nimport random\n\nif len(detection_results) > 0:\n    # Select a random keyframe with objects\n    valid_keyframes = [kf for kf in detection_results if len(kf['objects']) > 0]\n    \n    if valid_keyframes:\n        sample = random.choice(valid_keyframes)\n        keyframe_name = sample['keyframe']\n        objects = sample['objects']\n        \n        # Full path to keyframe\n        keyframe_dir = f\"{BATCH_NAME}/{video_name}\"\n        keyframe_path = os.path.join(keyframe_dir, keyframe_name)\n        \n        # Check if the image file exists\n        if os.path.exists(keyframe_path):\n            # Prepare detection results in the format needed by visualize_detection\n            vis_results = {\n                \"boxes\": [obj['box'] for obj in objects],\n                \"scores\": [obj['score'] for obj in objects],\n                \"labels\": [obj['object'] for obj in objects]\n            }\n            \n            # Visualize\n            print(f\"Visualizing keyframe: {keyframe_name}\")\n            print(f\"Objects detected: {[obj['object'] for obj in objects]}\")\n            \n            image = visualize_detection(keyframe_path, vis_results)\n            \n            if image is None:\n                print(\"Failed to visualize the image. Skipping visualization.\")\n        else:\n            print(f\"Keyframe file not found: {keyframe_path}\")\n            print(\"Skipping visualization.\")\n            print(f\"Objects detected: {[obj['object'] for obj in objects]}\")\n            print(\"Try using a different keyframe or check file paths.\")\n    else:\n        print(\"No keyframes with objects found.\")\nelse:\n    print(\"No detection results available to visualize.\")","metadata":{"_uuid":"1dfbc6ea-ef04-4f24-abee-1d34c0b256a3","_cell_guid":"9203e835-166a-47ed-86d1-f4d5d5e11b12","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 10. Zip detection results for download","metadata":{"_uuid":"d12aa029-ac53-4ae8-b93c-db84d6b36ed7","_cell_guid":"2467619c-244a-4c16-8ba2-957929c11157","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Zip all detection results for easy download\nimport shutil\nimport os\nfrom datetime import datetime\n\n# Get timestamp for unique filename\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\nzip_filename = f\"yolov8_detection_results_{timestamp}.zip\"\n\n# Check if we have detection results\nif os.path.exists('detection_results') and os.listdir('detection_results'):\n    file_count = len(os.listdir('detection_results'))\n    print(f\"Found {file_count} detection result files to zip\")\n    \n    # Create zip archive\n    shutil.make_archive(\n        base_name=zip_filename.split('.')[0],  # Remove .zip extension\n        format='zip',\n        root_dir='.',\n        base_dir='detection_results'\n    )\n    \n    # For Kaggle notebooks: Create a download link\n    try:\n        from IPython.display import HTML\n        import base64\n        \n        if os.path.exists(zip_filename):\n            file_size = os.path.getsize(zip_filename) / (1024 * 1024)  # Size in MB\n            print(f\"\\nZip file created: {zip_filename} ({file_size:.2f} MB)\")\n            \n            # In Kaggle, files can be found in the output section\n            print(\"\\nIn Kaggle: Find this file in the 'Output' tab of this notebook.\")\n        else:\n            print(\"Failed to create zip file\")\n            \n    except ImportError:\n        print(\"IPython display module not available.\")\nelse:\n    print(\"No detection results found to zip.\")\n    \nprint(\"\\nDetection process complete!\")","metadata":{"_uuid":"6556e9fd-6655-4ad9-85d2-8a94950b0fe9","_cell_guid":"77163216-ba66-46ae-a17e-e90371eb16b8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}