# %% [markdown]
# # Object Detection v·ªõi Grounding DINO
# 
# Notebook n√†y s·ª≠ d·ª•ng Grounding DINO ƒë·ªÉ th·ª±c hi·ªán object detection tr√™n c√°c keyframe t·ª´ video, s·ª≠ d·ª•ng caption ƒë√£ ƒë∆∞·ª£c tr√≠ch xu·∫•t tr∆∞·ªõc ƒë√≥.
# 
# ## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán

# %%
# C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
!pip install -q torch torchvision
!pip install -q transformers
!pip install -q timm
!pip install -q huggingface_hub
!pip install -q opencv-python-headless
!pip install -q gdown
!pip install -q matplotlib
!pip install -q Pillow
!pip install -q groundingdino-py

# %% [markdown]
# ## 2. T·∫£i d·ªØ li·ªáu t·ª´ Google Drive

# %%
# C·∫•u h√¨nh batch v√† ID Google Drive
BATCH_NAME = "L01"  # T√™n batch (L01, L02, L03, ...)
BATCH_ID = "14MeYV2WBWwldMDGRrpG9s7vz8triwbWr"  # ID cho L01.zip
BATCH_RESULT_ID = "15AVPGtZ6W3C3H8Hc_JF3SBsrhMVUbZWU"  # ID c·ªßa file results

# T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a c√≥
!mkdir -p data

# T·∫£i file batch (keyframes) t·ª´ Google Drive
print(f"T·∫£i xu·ªëng batch {BATCH_NAME}...")
!gdown {BATCH_ID} -O data/{BATCH_NAME}.zip
!unzip -qq data/{BATCH_NAME}.zip -d ./

# T·∫£i file k·∫øt qu·∫£ caption t·ª´ Google Drive
print(f"T·∫£i xu·ªëng k·∫øt qu·∫£ caption...")
!gdown {BATCH_RESULT_ID} -O data/results.zip
!unzip -qq data/results.zip -d data/

print("ƒê√£ t·∫£i xong d·ªØ li·ªáu!")

# %% [markdown]
# ## 3. Import th∆∞ vi·ªán

# %%
import torch
import json
import glob
import os
import cv2
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
from PIL import Image
import numpy as np

# X·ª≠ l√Ω k·∫øt n·ªëi Google Drive n·∫øu ch·∫°y trong Colab
try:
    from google.colab import drive
    try:
        drive.mount('/content/drive')
        print("[INFO] ƒê√£ k·∫øt n·ªëi Google Drive qua Colab")
    except NotImplementedError:
        print("[INFO] M√¥i tr∆∞·ªùng hi·ªán t·∫°i kh√¥ng h·ªó tr·ª£ mount Google Drive theo c√°ch c·ªßa Colab")
        # Trong Kaggle, d·ªØ li·ªáu ƒë∆∞·ª£c truy c·∫≠p tr·ª±c ti·∫øp t·ª´ th∆∞ m·ª•c hi·ªán t·∫°i
except ImportError:
    print("[INFO] Kh√¥ng ch·∫°y trong Colab, b·ªè qua mount Google Drive")

# Imports cho Grounding DINO s·∫Ω ƒë∆∞·ª£c th√™m v√†o ph·∫ßn t·∫£i model

# %% [markdown]
# ## 4. Ki·ªÉm tra GPU

# %%
# CUDA diagnosis section
print("\nGPU Diagnostic Information:")
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")
    print(f"Number of GPUs: {torch.cuda.device_count()}")
    print(f"GPU 0 name: {torch.cuda.get_device_name(0)}")
else:
    print("CUDA not available. Check NVIDIA drivers and PyTorch CUDA installation.")

# %% [markdown]
# ## 5. C·∫•u h√¨nh batch v√† ch·ªâ s·ªë video

# %%
# C·∫•u h√¨nh x·ª≠ l√Ω batch
START_VIDEO_INDEX = 1  # B·∫Øt ƒë·∫ßu t·ª´ V001
BATCH_SIZE = 2  # X·ª≠ l√Ω 2 video m·ªôt l·∫ßn

# ƒê·ªãnh nghƒ©a ƒë∆∞·ªùng d·∫´n
BATCH_PATH = BATCH_NAME  # V√≠ d·ª•: "L01"

# L·∫•y danh s√°ch video trong batch
videos = sorted(glob.glob(os.path.join(BATCH_PATH, "V*")))
print(f"T√¨m th·∫•y {len(videos)} th∆∞ m·ª•c video trong batch {BATCH_PATH}")

# Ch·ªâ x·ª≠ l√Ω c√°c video t·ª´ START_VIDEO_INDEX ƒë·∫øn START_VIDEO_INDEX + BATCH_SIZE - 1
end_idx = min(START_VIDEO_INDEX + BATCH_SIZE - 1, len(videos))
selected_videos = videos[START_VIDEO_INDEX - 1:end_idx]
print(f"X·ª≠ l√Ω {len(selected_videos)} video: {[os.path.basename(v) for v in selected_videos]}")

# %% [markdown]
# ## 6. T·∫£i model Grounding DINO

# %%
# T·∫£i model Grounding DINO
device = "cuda" if torch.cuda.is_available() else "cpu"

# S·ª≠ d·ª•ng groundingdino-py tr·ª±c ti·∫øp
from groundingdino.util.inference import load_model, load_image, predict, annotate
import groundingdino.datasets.transforms as T

# ƒê∆∞·ªùng d·∫´n cho model config v√† checkpoint
model_config_path = "GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py"
model_checkpoint_path = "weights/groundingdino_swint_ogc.pth"

# T·∫£i c·∫•u h√¨nh v√† checkpoint
!mkdir -p GroundingDINO/groundingdino/config
!mkdir -p weights
!wget -q -O {model_config_path} https://github.com/IDEA-Research/GroundingDINO/raw/main/groundingdino/config/GroundingDINO_SwinT_OGC.py
!wget -q -O {model_checkpoint_path} https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth

# T·∫£i model
model = load_model(model_config_path, model_checkpoint_path)
model.to(device)

print(f"ƒê√£ t·∫£i xong model Grounding DINO tr√™n device: {device}")

# %% [markdown]
# ## 7. H√†m h·ªó tr·ª£ ƒë·ªÉ tr√≠ch xu·∫•t ƒë·ªëi t∆∞·ª£ng t·ª´ caption

# %%
# H√†m tr√≠ch xu·∫•t c√°c ƒëo·∫°n c√≥ √Ω nghƒ©a t·ª´ caption ƒë·ªÉ l√†m prompt
def extract_objects_from_caption(caption):
    """
    Tr√≠ch xu·∫•t c√°c ƒëo·∫°n c√≥ √Ω nghƒ©a t·ª´ caption ƒë·ªÉ d√πng l√†m prompt cho object detection
    """
    # C·∫Øt b·ªè c√°c ph·∫ßn meta kh√¥ng c·∫ßn thi·∫øt
    caption = caption.replace("The image appears to be", "")
    caption = caption.replace("The image shows", "")
    
    # Danh s√°ch ƒë·ªÉ l∆∞u c√°c ƒëo·∫°n prompt
    prompts = []
    
    # Chia caption th√†nh c√°c ph·∫ßn theo d·∫•u ng·∫Øt d√≤ng
    parts = caption.split('\n')
    
    for part in parts:
        # B·ªè qua c√°c d√≤ng ng·∫Øn ho·∫∑c kh√¥ng c√≥ n·ªôi dung
        if len(part.strip()) < 10:
            continue
            
        # T√¨m c√°c ph·∫ßn m√¥ t·∫£ c·ª• th·ªÉ
        if ':' in part and '**' in part:
            # V√≠ d·ª•: "**Top Row:** - People walking..."
            topic_parts = part.split(':')
            if len(topic_parts) > 1 and len(topic_parts[1].strip()) > 10:
                prompts.append(topic_parts[1].strip())
        elif '-' in part:
            # Chia th√†nh c√°c ph·∫ßn theo d·∫•u g·∫°ch ngang
            bullet_points = part.split('-')
            for point in bullet_points:
                if len(point.strip()) > 10:
                    prompts.append(point.strip())
        elif len(part.strip()) > 20 and part.strip().endswith('.'):
            # L·∫•y c√°c c√¢u ho√†n ch·ªânh
            sentences = part.split('.')
            for sentence in sentences:
                if len(sentence.strip()) > 20:
                    prompts.append(sentence.strip() + '.')
    
    # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c prompt n√†o, d√πng caption g·ªëc
    if not prompts:
        # N·∫øu caption qu√° d√†i, chia th√†nh c√°c ƒëo·∫°n nh·ªè h∆°n
        if len(caption) > 200:
            sentences = caption.split('.')
            for sentence in sentences:
                if len(sentence.strip()) > 20:
                    prompts.append(sentence.strip() + '.')
        else:
            prompts.append(caption)
    
    # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng prompt ƒë·ªÉ kh√¥ng qu√° n·∫∑ng
    return prompts[:3]

# H√†m ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng v·ªõi Grounding DINO
def detect_objects(image_path, object_name, box_threshold=0.35, text_threshold=0.25):
    try:
        # T·∫£i v√† ti·ªÅn x·ª≠ l√Ω ·∫£nh
        image_source, image = load_image(image_path)
        
        # Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng
        boxes, logits, phrases = predict(
            model=model,
            image=image,
            caption=f"Find {object_name}",
            box_threshold=box_threshold,
            text_threshold=text_threshold,
            device=device
        )
        
        # Chuy·ªÉn ƒë·ªïi sang ƒë·ªãnh d·∫°ng th√¥ng th∆∞·ªùng
        H, W, _ = image_source.shape
        boxes_xyxy = boxes * torch.Tensor([W, H, W, H])
        boxes_xyxy = boxes_xyxy.cpu().numpy().tolist()
        
        return {
            "boxes": boxes_xyxy,
            "scores": logits.cpu().numpy().tolist(),
            "labels": phrases
        }
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω {os.path.basename(image_path)} v·ªõi ƒë·ªëi t∆∞·ª£ng '{object_name}': {e}")
        return {
            "boxes": [],
            "scores": [],
            "labels": []
        }

# H√†m v·∫Ω k·∫øt qu·∫£ l√™n ·∫£nh
def visualize_detection(image_path, detection_results):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    
    for box, score, label in zip(detection_results["boxes"], detection_results["scores"], detection_results["labels"]):
        x1, y1, x2, y2 = map(int, box)
        
        # V·∫Ω bounding box
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # V·∫Ω nh√£n v√† ƒëi·ªÉm s·ªë
        text = f"{label}: {score:.2f}"
        cv2.putText(image, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)
    
    plt.figure(figsize=(12, 8))
    plt.imshow(image)
    plt.axis('off')
    plt.show()
    
    return image

# %% [markdown]
# ## 8. X·ª≠ l√Ω v√† ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng

# %%
# T·∫°o th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
os.makedirs("detection_results", exist_ok=True)

# X·ª≠ l√Ω t·ª´ng video
for video_dir in tqdm(selected_videos, desc="X·ª≠ l√Ω video"):
    video_name = os.path.basename(video_dir)
    
    # T√¨m file caption t∆∞∆°ng ·ª©ng
    caption_file = os.path.join("data", "results", f"{BATCH_NAME}_{video_name}_caption.json")
    
    # Ki·ªÉm tra n·∫øu file caption t·ªìn t·∫°i
    if not os.path.exists(caption_file):
        print(f"[L·ªói] Kh√¥ng t√¨m th·∫•y file caption cho video {video_name}: {caption_file}")
        continue
    
    # ƒê·ªçc file caption
    with open(caption_file, 'r', encoding='utf-8') as f:
        captions = json.load(f)
    
    # Kh·ªüi t·∫°o danh s√°ch l∆∞u k·∫øt qu·∫£
    detection_results = []
    
    # X·ª≠ l√Ω t·ª´ng keyframe
    for item in tqdm(captions, desc=video_name):
        keyframe_name = item["keyframe"]
        caption = item["caption"]
        
        # ƒê∆∞·ªùng d·∫´n ƒë·∫ßy ƒë·ªß ƒë·∫øn file keyframe
        keyframe_path = os.path.join(video_dir, keyframe_name)
        
        # Tr√≠ch xu·∫•t c√°c ƒëo·∫°n prompt t·ª´ caption
        prompts = extract_objects_from_caption(caption)
        
        # Kh·ªüi t·∫°o k·∫øt qu·∫£ cho keyframe hi·ªán t·∫°i
        keyframe_results = {
            "keyframe": keyframe_name,
            "caption": caption,
            "objects": []
        }
        
        # Ph√°t hi·ªán v·ªõi t·ª´ng prompt
        for prompt in prompts:
            # Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng
            results = detect_objects(keyframe_path, prompt)
            
            # Th√™m k·∫øt qu·∫£ v√†o danh s√°ch
            for i, (box, score) in enumerate(zip(results["boxes"], results["scores"])):
                label = results["labels"][i] if i < len(results["labels"]) else prompt
                keyframe_results["objects"].append({
                    "prompt": prompt,  # L∆∞u l·∫°i prompt ƒë√£ s·ª≠ d·ª•ng
                    "object": label,
                    "box": box,
                    "score": score
                })
        
        # Th√™m k·∫øt qu·∫£ c·ªßa keyframe v√†o danh s√°ch chung
        detection_results.append(keyframe_results)
    
    # L∆∞u k·∫øt qu·∫£ ph√°t hi·ªán v√†o file JSON
    output_file = os.path.join("detection_results", f"{BATCH_NAME}_{video_name}_detection.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(detection_results, f, indent=2, ensure_ascii=False)
    
    print(f"[OK] ƒê√£ l∆∞u k·∫øt qu·∫£ ph√°t hi·ªán cho video {video_name} v√†o file {output_file}")

# %% [markdown]
# ## 9. N√©n k·∫øt qu·∫£ ƒë·ªÉ t·∫£i v·ªÅ

# %%
# N√©n th∆∞ m·ª•c k·∫øt qu·∫£ ƒë·ªÉ t·∫£i v·ªÅ
!zip -r detection_results.zip detection_results

print("‚úÖ ƒê√£ n√©n k·∫øt qu·∫£ v√†o file detection_results.zip")
print("üëâ B·∫°n c√≥ th·ªÉ t·∫£i file n√†y v·ªÅ t·ª´ panel Files ·ªü b√™n tr√°i.")
