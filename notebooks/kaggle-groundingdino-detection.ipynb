{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Object Detection with Grounding DINO\n\nThis notebook uses Grounding DINO to perform object detection on keyframes from videos, using previously extracted captions.\n\n## 1. Install required libraries","metadata":{"_uuid":"487e1847-c874-4153-855b-da4961f11de5","_cell_guid":"e00a7d2e-c42a-486e-9a64-e895059607a4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Install required libraries\n!pip install -q torch torchvision\n!pip install -q transformers\n!pip install -q timm\n!pip install -q huggingface_hub\n!pip install -q opencv-python-headless\n!pip install -q gdown\n!pip install -q matplotlib\n!pip install -q Pillow\n!pip install -q groundingdino-py","metadata":{"_uuid":"ff18b975-041f-4367-9c54-1add6430da52","_cell_guid":"6809a7a0-9a39-43e5-9c76-994432f285db","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-19T17:17:45.495047Z","iopub.execute_input":"2025-07-19T17:17:45.495266Z","iopub.status.idle":"2025-07-19T17:19:24.597649Z","shell.execute_reply.started":"2025-07-19T17:17:45.495249Z","shell.execute_reply":"2025-07-19T17:19:24.596914Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.8/46.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.2/256.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for groundingdino-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 2. Download data from Google Drive","metadata":{"_uuid":"49fe9e8f-183f-4e47-bc25-09b4d8c540ba","_cell_guid":"99ebf0d4-1162-4ee7-aee7-a4b6687d5d15","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Configure batch and Google Drive IDs\nBATCH_NAME = \"L01\"  # Batch name (L01, L02, L03, ...)\nBATCH_ID = \"14MeYV2WBWwldMDGRrpG9s7vz8triwbWr\"  # ID for L01.zip\nBATCH_RESULT_ID = \"15AVPGtZ6W3C3H8Hc_JF3SBsrhMVUbZWU\"  # ID for results file\n\n# Create data directory if it doesn't exist\n!mkdir -p data\n\n# Download batch (keyframes) from Google Drive\nprint(f\"Downloading batch {BATCH_NAME}...\")\n!gdown {BATCH_ID} -O data/{BATCH_NAME}.zip\n!unzip -qq data/{BATCH_NAME}.zip -d ./\n\n# Download results file with captions\nprint(f\"Downloading caption results...\")\n!gdown {BATCH_RESULT_ID} -O data/results.zip\n!unzip -qq data/results.zip -d data/\n\nprint(\"Data downloaded successfully!\")\n\n# Create directory for detection results\n!mkdir -p detection_results\n\nprint(\"Data downloaded successfully!\")","metadata":{"_uuid":"ed5f1e47-4c18-49eb-8292-6b2bfa548d6f","_cell_guid":"f0a6a674-5d07-4f1d-b1d3-9a821afeef4d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-19T17:19:27.923032Z","iopub.execute_input":"2025-07-19T17:19:27.923282Z","iopub.status.idle":"2025-07-19T17:19:43.585422Z","shell.execute_reply.started":"2025-07-19T17:19:27.923254Z","shell.execute_reply":"2025-07-19T17:19:43.584601Z"}},"outputs":[{"name":"stdout","text":"Downloading batch L01...\nDownloading...\nFrom (original): https://drive.google.com/uc?id=14MeYV2WBWwldMDGRrpG9s7vz8triwbWr\nFrom (redirected): https://drive.google.com/uc?id=14MeYV2WBWwldMDGRrpG9s7vz8triwbWr&confirm=t&uuid=30d1a7d6-3851-46e9-bbf7-2082cc49b066\nTo: /kaggle/working/data/L01.zip\n100%|████████████████████████████████████████| 527M/527M [00:05<00:00, 91.7MB/s]\nDownloading caption results...\nDownloading...\nFrom: https://drive.google.com/uc?id=15AVPGtZ6W3C3H8Hc_JF3SBsrhMVUbZWU\nTo: /kaggle/working/data/results.zip\n100%|█████████████████████████████████████████| 599k/599k [00:00<00:00, 111MB/s]\nData downloaded successfully!\nData downloaded successfully!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 3. Import libraries","metadata":{"_uuid":"64aa5268-d6a7-4b81-9c19-36f81834695e","_cell_guid":"98ce4bf1-fce3-456e-855d-7216d75c466c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Import required libraries\nimport os\nimport json\nimport glob\nimport torch\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom PIL import Image\nimport numpy as np\n\n# Handle Google Drive connection if running in Colab\ntry:\n    from google.colab import drive\n    try:\n        drive.mount('/content/drive')\n        print(\"[INFO] Google Drive connected via Colab\")\n    except NotImplementedError:\n        print(\"[INFO] Current environment does not support mounting Google Drive via Colab\")\n        # In Kaggle, data is accessed directly from the current directory\nexcept ImportError:\n    print(\"[INFO] Not running in Colab, skipping Google Drive mount\")\n\n# Imports for Grounding DINO will be added in the model loading section","metadata":{"_uuid":"f7b2135a-23e8-46eb-82b4-c15b20b49142","_cell_guid":"a8836c3d-8def-444b-9176-bc92e5ed9784","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-19T17:19:47.373799Z","iopub.execute_input":"2025-07-19T17:19:47.374546Z","iopub.status.idle":"2025-07-19T17:19:51.087875Z","shell.execute_reply.started":"2025-07-19T17:19:47.374508Z","shell.execute_reply":"2025-07-19T17:19:51.087127Z"}},"outputs":[{"name":"stdout","text":"[INFO] Current environment does not support mounting Google Drive via Colab\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 4. Check environment","metadata":{"_uuid":"490d78a5-ffe3-4195-b839-28e03bfbb5eb","_cell_guid":"69c35d05-e933-449c-8b70-552f546ad435","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Check GPU\nif torch.cuda.is_available():\n    device_count = torch.cuda.device_count()\n    current_device = torch.cuda.current_device()\n    device_name = torch.cuda.get_device_name(current_device)\n    \n    print(f\"Number of GPUs: {device_count}\")\n    print(f\"Current GPU: {current_device}\")\n    print(f\"GPU name: {device_name}\")\n    print(f\"CUDA version: {torch.version.cuda}\")\nelse:\n    print(\"CUDA not available. Check NVIDIA drivers and PyTorch CUDA installation.\")","metadata":{"_uuid":"aaeca276-152e-4a68-a965-b23a1534c024","_cell_guid":"5fec78ee-971f-46dc-a8fd-3404aad769e1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-19T17:19:59.049566Z","iopub.execute_input":"2025-07-19T17:19:59.049926Z","iopub.status.idle":"2025-07-19T17:19:59.175139Z","shell.execute_reply.started":"2025-07-19T17:19:59.049903Z","shell.execute_reply":"2025-07-19T17:19:59.174421Z"}},"outputs":[{"name":"stdout","text":"Number of GPUs: 2\nCurrent GPU: 0\nGPU name: Tesla T4\nCUDA version: 12.4\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 5. Configure batch and video indices","metadata":{"_uuid":"c23162b5-6f4f-475e-980c-404a9c7be6cd","_cell_guid":"754c6a52-ca6f-46a7-b019-85394f3914ed","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Configure batch processing\nSTART_VIDEO_INDEX = 1  # Start from V001\nBATCH_SIZE = 8  # Process 8 videos at a time\n\n# Define paths\nBATCH_PATH = BATCH_NAME  # Example: \"L01\"\n\n# Get list of videos in the batch\nvideos = sorted(glob.glob(os.path.join(BATCH_PATH, \"V*\")))\n\n# Define paths\nBATCH_PATH = BATCH_NAME  # Example: \"L01\"\n\n# Get list of videos in the batch\nvideos = sorted(glob.glob(os.path.join(BATCH_PATH, \"V*\")))\nprint(f\"Found {len(videos)} video directories in batch {BATCH_PATH}\")\n\n# Only process videos from START_VIDEO_INDEX to START_VIDEO_INDEX + BATCH_SIZE - 1\nend_idx = min(START_VIDEO_INDEX + BATCH_SIZE - 1, len(videos))\nselected_videos = videos[START_VIDEO_INDEX - 1:end_idx]\nprint(f\"Processing {len(selected_videos)} videos: {[os.path.basename(v) for v in selected_videos]}\")","metadata":{"_uuid":"f69e328d-ec8a-480c-b3f9-d464d860bcac","_cell_guid":"918f5bc7-b609-4b70-bb9b-9b0afe2a393f","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-19T17:20:01.889053Z","iopub.execute_input":"2025-07-19T17:20:01.889547Z","iopub.status.idle":"2025-07-19T17:20:01.896040Z","shell.execute_reply.started":"2025-07-19T17:20:01.889522Z","shell.execute_reply":"2025-07-19T17:20:01.895322Z"}},"outputs":[{"name":"stdout","text":"Found 8 video directories in batch L01\nProcessing 8 videos: ['V001', 'V002', 'V003', 'V004', 'V005', 'V006', 'V007', 'V008']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 6. Load Grounding DINO model","metadata":{"_uuid":"4fd19564-db1a-4c71-baf1-3b8aed43c739","_cell_guid":"6b4d4c2a-832d-4564-8dbc-1b90cd6c3ad1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Load Grounding DINO model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Use groundingdino-py directly\nfrom groundingdino.util.inference import load_model, load_image, predict, annotate\nimport groundingdino.datasets.transforms as T\n\n# Paths for model config and checkpoint\nmodel_config_path = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\nmodel_checkpoint_path = \"weights/groundingdino_swint_ogc.pth\"\n\n# Download config and checkpoint\n!mkdir -p GroundingDINO/groundingdino/config\n!mkdir -p weights\n!wget -q -O {model_config_path} https://github.com/IDEA-Research/GroundingDINO/raw/main/groundingdino/config/GroundingDINO_SwinT_OGC.py\n!wget -q -O {model_checkpoint_path} https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n\n# Load model\nmodel = load_model(model_config_path, model_checkpoint_path)\nmodel.to(device)\n\nprint(f\"Grounding DINO model loaded on device: {device}\")","metadata":{"_uuid":"463f292b-13ec-4870-bd6e-c84e8217433d","_cell_guid":"a353d32d-3319-45b6-afa2-df16275563bd","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-19T17:20:06.117562Z","iopub.execute_input":"2025-07-19T17:20:06.118066Z","iopub.status.idle":"2025-07-19T17:20:43.583883Z","shell.execute_reply.started":"2025-07-19T17:20:06.118031Z","shell.execute_reply":"2025-07-19T17:20:43.583035Z"}},"outputs":[{"name":"stderr","text":"2025-07-19 17:20:15.259894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752945615.448073      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752945615.508158      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n/usr/local/lib/python3.11/dist-packages/torch/functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /pytorch/aten/src/ATen/native/TensorShape.cpp:3637.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"},{"name":"stdout","text":"final text_encoder_type: bert-base-uncased\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc4de3b3ffed4217aa3542302be43d0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceecc743e135419fbddeb8a42ce457f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2792fbe0f024d5c869878bd4b8353bc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d676f696176475aaf2e8c0f70e568cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4db1ec7fc8419794ce19003972320b"}},"metadata":{}},{"name":"stdout","text":"Grounding DINO model loaded on device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 7. Helper functions for extracting objects from captions","metadata":{"_uuid":"950703b0-3462-4885-9aff-6f80823a03e3","_cell_guid":"ed367dac-e9e7-4185-ad2e-b955cbdca4f4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Function to extract meaningful segments from captions to use as prompts\ndef extract_objects_from_caption(caption):\n    \"\"\"\n    Extract meaningful segments from captions to use as prompts for object detection\n    \"\"\"\n    # Remove unnecessary meta text\n    caption = caption.replace(\"The image appears to be\", \"\")\n    caption = caption.replace(\"The image shows\", \"\")\n    \n    # List to store prompt segments\n    prompts = []\n    \n    # Split caption into parts by line breaks\n    parts = caption.split('\\n')\n    \n    for part in parts:\n        # Skip short lines or lines without content\n        if len(part.strip()) < 10:\n            continue\n            \n        # Find specific description sections\n        if ':' in part and '**' in part:\n            # Example: \"**Top Row:** - People walking...\"\n            topic_parts = part.split(':')\n            if len(topic_parts) > 1 and len(topic_parts[1].strip()) > 10:\n                prompts.append(topic_parts[1].strip())\n        elif '-' in part:\n            # Split into parts by hyphens\n            bullet_points = part.split('-')\n            for point in bullet_points:\n                if len(point.strip()) > 10:\n                    prompts.append(point.strip())\n        elif len(part.strip()) > 20 and part.strip().endswith('.'):\n            # Get complete sentences\n            sentences = part.split('.')\n            for sentence in sentences:\n                if len(sentence.strip()) > 20:\n                    prompts.append(sentence.strip() + '.')\n    \n    # If no prompts were found, use the original caption\n    if not prompts:\n        # If caption is too long, split into smaller segments\n        if len(caption) > 200:\n            sentences = caption.split('.')\n            for sentence in sentences:\n                if len(sentence.strip()) > 20:\n                    prompts.append(sentence.strip() + '.')\n        else:\n            prompts.append(caption)\n    \n    # Limit the number of prompts to avoid overload\n    return prompts[:3]\n\n# Function to detect objects with Grounding DINO\ndef detect_objects(image_path, object_name, box_threshold=0.35, text_threshold=0.25):\n    try:\n        # Load and preprocess image\n        image_source, image = load_image(image_path)\n        \n        # Detect objects\n        boxes, logits, phrases = predict(\n            model=model,\n            image=image,\n            caption=f\"Find {object_name}\",\n            box_threshold=box_threshold,\n            text_threshold=text_threshold,\n            device=device\n        )\n        \n        # Convert to standard format\n        H, W, _ = image_source.shape\n        boxes_xyxy = boxes * torch.Tensor([W, H, W, H])\n        boxes_xyxy = boxes_xyxy.cpu().numpy().tolist()\n        \n        return {\n            \"boxes\": boxes_xyxy,\n            \"scores\": logits.cpu().numpy().tolist(),\n            \"labels\": phrases\n        }\n    except Exception as e:\n        print(f\"Error processing {os.path.basename(image_path)} with object '{object_name}': {e}\")\n        return {\n            \"boxes\": [],\n            \"scores\": [],\n            \"labels\": []\n        }\n\n# Function to visualize detection results on an image\ndef visualize_detection(image_path, detection_results):\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n    for box, score, label in zip(detection_results[\"boxes\"], detection_results[\"scores\"], detection_results[\"labels\"]):\n        x1, y1, x2, y2 = map(int, box)\n        \n        # Draw bounding box\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n        \n        # Draw label and score\n        text = f\"{label}: {score:.2f}\"\n        cv2.putText(image, text, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n    \n    plt.figure(figsize=(12, 8))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n    \n    return image\n\n# Function to calculate IoU (Intersection over Union)\ndef calculate_iou(box1, box2):\n    \"\"\"Calculate IoU between two bounding boxes\"\"\"\n    # Box coordinates\n    x1_1, y1_1, x2_1, y2_1 = box1\n    x1_2, y1_2, x2_2, y2_2 = box2\n    \n    # Calculate area of each box\n    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n    \n    # Calculate coordinates of intersection\n    x1_i = max(x1_1, x1_2)\n    y1_i = max(y1_1, y1_2)\n    x2_i = min(x2_1, x2_2)\n    y2_i = min(y2_1, y2_2)\n    \n    # Check if there is no intersection\n    if x2_i < x1_i or y2_i < y1_i:\n        return 0.0\n    \n    # Calculate area of intersection\n    area_intersection = (x2_i - x1_i) * (y2_i - y1_i)\n    \n    # Calculate IoU\n    iou = area_intersection / (area1 + area2 - area_intersection)\n    \n    return iou\n\n# Function to filter duplicate objects and keep only the highest scoring object\ndef filter_objects(objects, iou_threshold=0.7, confidence_threshold=0.5):\n    \"\"\"Filter duplicated objects, keep only the highest scoring object for each group of overlapping boxes\"\"\"\n    # If there are no objects, return empty list\n    if not objects:\n        return []\n    \n    # Filter objects based on confidence threshold\n    objects = [obj for obj in objects if obj[\"score\"] >= confidence_threshold]\n    \n    # Sort objects by score in descending order\n    sorted_objects = sorted(objects, key=lambda x: x[\"score\"], reverse=True)\n    \n    # List to store filtered objects\n    filtered_objects = []\n    \n    # Iterate through each object\n    for obj in sorted_objects:\n        # Check if current object overlaps with any object in filtered_objects\n        duplicate = False\n        for filtered_obj in filtered_objects:\n            # If same object name and IoU greater than threshold\n            if obj[\"object\"] == filtered_obj[\"object\"] and \\\n               calculate_iou(obj[\"box\"], filtered_obj[\"box\"]) > iou_threshold:\n                duplicate = True\n                break\n        \n        # If not duplicate, add to filtered list\n        if not duplicate:\n            filtered_objects.append(obj)\n    \n    return filtered_objects","metadata":{"_uuid":"ced19e21-6608-4978-884e-0dd299b8ca69","_cell_guid":"932cd061-8e65-417f-936d-539e7ae1f675","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-19T17:20:57.969552Z","iopub.execute_input":"2025-07-19T17:20:57.970302Z","iopub.status.idle":"2025-07-19T17:20:57.985554Z","shell.execute_reply.started":"2025-07-19T17:20:57.970275Z","shell.execute_reply":"2025-07-19T17:20:57.984933Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## 8. Processing and object detection","metadata":{"_uuid":"3107c97e-370f-4752-9bba-dd7a9bf071f6","_cell_guid":"54110b02-0ea7-4f4d-9e93-826c40d10c54","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Create directory to save results\n!mkdir -p detection_results\n\n# Process each video\nfor video_dir in selected_videos:\n    video_name = os.path.basename(video_dir)\n    print(f\"\\nProcessing video: {video_name}\")\n    \n    # Path to the caption file, based on confirmed directory structure\n    caption_file = os.path.join(\"data\", \"results\", f\"{BATCH_NAME}_{video_name}_caption.json\")\n    \n    try:\n        with open(caption_file, 'r', encoding='utf-8') as f:\n            captions = json.load(f)\n        print(f\"Successfully loaded {len(captions)} keyframes from {caption_file}\")\n    except FileNotFoundError:\n        print(f\"Error: Caption file not found at {caption_file}\")\n        continue\n    except Exception as e:\n        print(f\"Error reading caption file {caption_file}: {e}\")\n        continue\n    \n    # Initialize list to store results\n    detection_results = []\n    \n    # Process each keyframe\n    for item in captions:\n        keyframe_name = item[\"keyframe\"]\n        caption = item[\"caption\"]\n        \n        # Full path to the keyframe file. video_dir is the correct path (e.g., 'L01/V001')\n        keyframe_path = os.path.join(video_dir, keyframe_name)\n        \n        # Extract prompt segments from caption\n        prompts = extract_objects_from_caption(caption)\n        \n        # Initialize results for current keyframe\n        keyframe_results = {\n            \"keyframe\": keyframe_name,\n            \"caption\": caption,\n            \"objects\": []\n        }\n        \n        # Detect objects using each prompt\n        for prompt in prompts:\n            # Detect objects\n            results = detect_objects(keyframe_path, prompt)\n            \n            # Add results to the list\n            for i, (box, score) in enumerate(zip(results[\"boxes\"], results[\"scores\"])):\n                label = results[\"labels\"][i] if i < len(results[\"labels\"]) else prompt\n                keyframe_results[\"objects\"].append({\n                    \"prompt\": prompt,  # Save the prompt that was used\n                    \"object\": label,\n                    \"box\": box,\n                    \"score\": score\n                })\n        \n        # Apply filter_objects to remove duplicate objects and filter by score\n        keyframe_results[\"objects\"] = filter_objects(keyframe_results[\"objects\"])\n        \n        print(f\"Keyframe {keyframe_name}: {len(keyframe_results['objects'])} objects after filtering\")\n        \n        # Add keyframe results to the main list\n        detection_results.append(keyframe_results)\n    \n    # Save results to JSON file\n    output_file = os.path.join(\"detection_results\", f\"{BATCH_NAME}_{video_name}_detection.json\")\n    with open(output_file, 'w', encoding='utf-8') as f:\n        json.dump(detection_results, f, ensure_ascii=False, indent=4)\n    \n    print(f\"\\nSaved detection results for {video_name} to {output_file}\")\n\nprint(\"\\nObject detection completed for all videos!\")","metadata":{"_uuid":"e4201557-71ac-4b59-87a9-a1da173114c5","_cell_guid":"e594b814-a761-437a-a144-3df2c7f468b6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-19T17:21:02.367032Z","iopub.execute_input":"2025-07-19T17:21:02.367572Z","execution_failed":"2025-07-19T17:25:09.092Z"}},"outputs":[{"name":"stdout","text":"\nProcessing video: V001\nSuccessfully loaded 803 keyframes from data/results/L01_V001_caption.json\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:1614: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/groundingdino/models/GroundingDINO/transformer.py:862: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast(enabled=False):\n","output_type":"stream"},{"name":"stdout","text":"Keyframe L01_V001_000000.jpg: 0 objects after filtering\nKeyframe L01_V001_000009.jpg: 3 objects after filtering\nKeyframe L01_V001_000019.jpg: 0 objects after filtering\nKeyframe L01_V001_000020.jpg: 3 objects after filtering\nKeyframe L01_V001_000035.jpg: 5 objects after filtering\nKeyframe L01_V001_000050.jpg: 1 objects after filtering\nKeyframe L01_V001_000052.jpg: 0 objects after filtering\nKeyframe L01_V001_000260.jpg: 13 objects after filtering\nKeyframe L01_V001_000469.jpg: 1 objects after filtering\nKeyframe L01_V001_000470.jpg: 1 objects after filtering\nKeyframe L01_V001_000490.jpg: 1 objects after filtering\nKeyframe L01_V001_000510.jpg: 1 objects after filtering\nKeyframe L01_V001_000511.jpg: 0 objects after filtering\nKeyframe L01_V001_000529.jpg: 0 objects after filtering\nKeyframe L01_V001_000547.jpg: 0 objects after filtering\nKeyframe L01_V001_000548.jpg: 2 objects after filtering\nKeyframe L01_V001_000568.jpg: 0 objects after filtering\nKeyframe L01_V001_000588.jpg: 1 objects after filtering\nKeyframe L01_V001_000589.jpg: 2 objects after filtering\nKeyframe L01_V001_000605.jpg: 2 objects after filtering\nKeyframe L01_V001_000622.jpg: 3 objects after filtering\nKeyframe L01_V001_000623.jpg: 3 objects after filtering\nKeyframe L01_V001_000647.jpg: 1 objects after filtering\nKeyframe L01_V001_000672.jpg: 1 objects after filtering\nKeyframe L01_V001_000673.jpg: 8 objects after filtering\nKeyframe L01_V001_000692.jpg: 6 objects after filtering\nKeyframe L01_V001_000711.jpg: 3 objects after filtering\nKeyframe L01_V001_000712.jpg: 5 objects after filtering\nKeyframe L01_V001_000726.jpg: 5 objects after filtering\nKeyframe L01_V001_000741.jpg: 6 objects after filtering\nKeyframe L01_V001_000742.jpg: 4 objects after filtering\nKeyframe L01_V001_000758.jpg: 5 objects after filtering\nKeyframe L01_V001_000774.jpg: 2 objects after filtering\nKeyframe L01_V001_000775.jpg: 3 objects after filtering\nKeyframe L01_V001_000798.jpg: 3 objects after filtering\nKeyframe L01_V001_000822.jpg: 3 objects after filtering\nKeyframe L01_V001_000823.jpg: 3 objects after filtering\nKeyframe L01_V001_000845.jpg: 2 objects after filtering\nKeyframe L01_V001_000867.jpg: 6 objects after filtering\nKeyframe L01_V001_000868.jpg: 1 objects after filtering\nKeyframe L01_V001_000896.jpg: 1 objects after filtering\nKeyframe L01_V001_000924.jpg: 1 objects after filtering\nKeyframe L01_V001_000925.jpg: 1 objects after filtering\nKeyframe L01_V001_000946.jpg: 19 objects after filtering\nKeyframe L01_V001_000967.jpg: 21 objects after filtering\nKeyframe L01_V001_000968.jpg: 10 objects after filtering\nKeyframe L01_V001_000984.jpg: 5 objects after filtering\nKeyframe L01_V001_001001.jpg: 5 objects after filtering\nKeyframe L01_V001_001002.jpg: 14 objects after filtering\nKeyframe L01_V001_001028.jpg: 7 objects after filtering\nKeyframe L01_V001_001055.jpg: 10 objects after filtering\nKeyframe L01_V001_001056.jpg: 0 objects after filtering\nKeyframe L01_V001_001068.jpg: 1 objects after filtering\nKeyframe L01_V001_001081.jpg: 0 objects after filtering\nKeyframe L01_V001_001082.jpg: 7 objects after filtering\nKeyframe L01_V001_001125.jpg: 1 objects after filtering\nKeyframe L01_V001_001168.jpg: 2 objects after filtering\nKeyframe L01_V001_001169.jpg: 8 objects after filtering\nKeyframe L01_V001_001457.jpg: 7 objects after filtering\nKeyframe L01_V001_001745.jpg: 4 objects after filtering\nKeyframe L01_V001_001746.jpg: 10 objects after filtering\nKeyframe L01_V001_001898.jpg: 0 objects after filtering\nKeyframe L01_V001_002050.jpg: 3 objects after filtering\nKeyframe L01_V001_002051.jpg: 3 objects after filtering\nKeyframe L01_V001_002843.jpg: 2 objects after filtering\nKeyframe L01_V001_003635.jpg: 1 objects after filtering\nKeyframe L01_V001_003636.jpg: 5 objects after filtering\nKeyframe L01_V001_003838.jpg: 8 objects after filtering\nKeyframe L01_V001_004041.jpg: 7 objects after filtering\nKeyframe L01_V001_004042.jpg: 2 objects after filtering\nKeyframe L01_V001_004157.jpg: 1 objects after filtering\nKeyframe L01_V001_004273.jpg: 4 objects after filtering\nKeyframe L01_V001_004274.jpg: 3 objects after filtering\nKeyframe L01_V001_004405.jpg: 0 objects after filtering\nKeyframe L01_V001_004537.jpg: 2 objects after filtering\nKeyframe L01_V001_004538.jpg: 0 objects after filtering\nKeyframe L01_V001_004599.jpg: 0 objects after filtering\nKeyframe L01_V001_004660.jpg: 0 objects after filtering\nKeyframe L01_V001_004661.jpg: 14 objects after filtering\nKeyframe L01_V001_004769.jpg: 4 objects after filtering\nKeyframe L01_V001_004878.jpg: 1 objects after filtering\nKeyframe L01_V001_004879.jpg: 1 objects after filtering\nKeyframe L01_V001_005054.jpg: 2 objects after filtering\nKeyframe L01_V001_005229.jpg: 1 objects after filtering\nKeyframe L01_V001_005230.jpg: 9 objects after filtering\nKeyframe L01_V001_005305.jpg: 4 objects after filtering\nKeyframe L01_V001_005381.jpg: 2 objects after filtering\nKeyframe L01_V001_005382.jpg: 1 objects after filtering\nKeyframe L01_V001_005418.jpg: 1 objects after filtering\nKeyframe L01_V001_005454.jpg: 0 objects after filtering\nKeyframe L01_V001_005455.jpg: 6 objects after filtering\nKeyframe L01_V001_005602.jpg: 4 objects after filtering\nKeyframe L01_V001_005749.jpg: 5 objects after filtering\nKeyframe L01_V001_005750.jpg: 2 objects after filtering\nKeyframe L01_V001_005871.jpg: 0 objects after filtering\nKeyframe L01_V001_005993.jpg: 0 objects after filtering\nKeyframe L01_V001_005994.jpg: 3 objects after filtering\nKeyframe L01_V001_006059.jpg: 2 objects after filtering\nKeyframe L01_V001_006124.jpg: 3 objects after filtering\nKeyframe L01_V001_006125.jpg: 2 objects after filtering\nKeyframe L01_V001_006265.jpg: 0 objects after filtering\nKeyframe L01_V001_006406.jpg: 2 objects after filtering\nKeyframe L01_V001_006407.jpg: 1 objects after filtering\nKeyframe L01_V001_006520.jpg: 1 objects after filtering\nKeyframe L01_V001_006633.jpg: 2 objects after filtering\nKeyframe L01_V001_006634.jpg: 4 objects after filtering\nKeyframe L01_V001_006827.jpg: 2 objects after filtering\nKeyframe L01_V001_007021.jpg: 5 objects after filtering\nKeyframe L01_V001_007022.jpg: 3 objects after filtering\nKeyframe L01_V001_007215.jpg: 1 objects after filtering\nKeyframe L01_V001_007409.jpg: 1 objects after filtering\nKeyframe L01_V001_007410.jpg: 2 objects after filtering\nKeyframe L01_V001_007457.jpg: 1 objects after filtering\nKeyframe L01_V001_007505.jpg: 2 objects after filtering\nKeyframe L01_V001_007506.jpg: 0 objects after filtering\nKeyframe L01_V001_007540.jpg: 2 objects after filtering\nKeyframe L01_V001_007575.jpg: 0 objects after filtering\nKeyframe L01_V001_007576.jpg: 3 objects after filtering\nKeyframe L01_V001_007710.jpg: 1 objects after filtering\nKeyframe L01_V001_007845.jpg: 1 objects after filtering\nKeyframe L01_V001_007846.jpg: 7 objects after filtering\nKeyframe L01_V001_008126.jpg: 8 objects after filtering\nKeyframe L01_V001_008407.jpg: 7 objects after filtering\nKeyframe L01_V001_008408.jpg: 7 objects after filtering\nKeyframe L01_V001_008440.jpg: 6 objects after filtering\nKeyframe L01_V001_008472.jpg: 5 objects after filtering\nKeyframe L01_V001_008473.jpg: 5 objects after filtering\nKeyframe L01_V001_008486.jpg: 8 objects after filtering\nKeyframe L01_V001_008500.jpg: 10 objects after filtering\nKeyframe L01_V001_008501.jpg: 18 objects after filtering\nKeyframe L01_V001_008537.jpg: 18 objects after filtering\nKeyframe L01_V001_008574.jpg: 21 objects after filtering\nKeyframe L01_V001_008575.jpg: 6 objects after filtering\nKeyframe L01_V001_008605.jpg: 4 objects after filtering\nKeyframe L01_V001_008635.jpg: 3 objects after filtering\nKeyframe L01_V001_008636.jpg: 4 objects after filtering\nKeyframe L01_V001_008669.jpg: 12 objects after filtering\nKeyframe L01_V001_008703.jpg: 3 objects after filtering\nKeyframe L01_V001_008704.jpg: 11 objects after filtering\nKeyframe L01_V001_008737.jpg: 9 objects after filtering\nKeyframe L01_V001_008770.jpg: 4 objects after filtering\nKeyframe L01_V001_008771.jpg: 4 objects after filtering\nKeyframe L01_V001_008805.jpg: 4 objects after filtering\nKeyframe L01_V001_008839.jpg: 5 objects after filtering\nKeyframe L01_V001_008840.jpg: 6 objects after filtering\nKeyframe L01_V001_008867.jpg: 16 objects after filtering\nKeyframe L01_V001_008895.jpg: 2 objects after filtering\nKeyframe L01_V001_008896.jpg: 1 objects after filtering\nKeyframe L01_V001_009546.jpg: 1 objects after filtering\nKeyframe L01_V001_010197.jpg: 1 objects after filtering\nKeyframe L01_V001_010198.jpg: 5 objects after filtering\nKeyframe L01_V001_010472.jpg: 4 objects after filtering\nKeyframe L01_V001_010746.jpg: 7 objects after filtering\nKeyframe L01_V001_010747.jpg: 5 objects after filtering\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## 9. Compress results for download","metadata":{"_uuid":"33729b20-b396-4ee3-a4f7-3a6151c3541b","_cell_guid":"64bf3b4d-0137-4216-89c5-c019530f920d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# Compress results for download\n!cd detection_results && zip -r ../detection_results.zip *\nprint(\"\\nCreated detection_results.zip file for detection results\")\n\n# In Colab, you can download this file by clicking on the folder icon on the left","metadata":{"_uuid":"b82cfdc6-2db1-4b3b-8e74-a9475aa78ee7","_cell_guid":"b212cb1c-062a-42d9-a5d0-c564fb14d42c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}